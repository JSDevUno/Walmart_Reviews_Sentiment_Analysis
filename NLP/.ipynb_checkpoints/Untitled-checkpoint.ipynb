{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6f06b-2daf-46e0-a396-b8e616478939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import (Embedding, LSTM, Bidirectional, Dense, \n",
    "                                          Dropout, Input, Concatenate, GlobalMaxPooling1D)\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"Warning: TensorFlow not installed. Install with: pip install tensorflow\")\n",
    "\n",
    "\n",
    "class BiLSTMContextTrainer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize BiLSTM with context and tone awareness\"\"\"\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.max_len = 150  # Optimal for reviews\n",
    "        self.vocab_size = 10000\n",
    "        self.embedding_dim = 100\n",
    "        \n",
    "        self.label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "        self.reverse_label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "        \n",
    "        # Tone and context markers\n",
    "        self.intensity_amplifiers = {\n",
    "            'very', 'extremely', 'incredibly', 'absolutely', 'totally',\n",
    "            'completely', 'utterly', 'highly', 'really', 'so', 'super'\n",
    "        }\n",
    "        \n",
    "        self.negation_words = {\n",
    "            'not', 'no', 'never', 'nothing', 'nowhere', 'neither', 'nobody',\n",
    "            'none', 'hardly', 'scarcely', 'barely', \"n't\", 'cannot', 'cant', 'won\\'t'\n",
    "        }\n",
    "        \n",
    "        self.positive_emoticons = [':)', ':-)', ':D', ':-D', ':P', '^_^', 'üòä', 'üòÉ', 'üëç', '‚ù§Ô∏è']\n",
    "        self.negative_emoticons = [':(', ':-(', ':[', ':-[', ':/',':-/', 'üò¢', 'üòû', 'üëé', 'üíî']\n",
    "    \n",
    "    def extract_contextual_features(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Extract tone and context features for BiLSTM\"\"\"\n",
    "        features = []\n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        # Tone indicators\n",
    "        features.append(text.count('!') / max(len(text), 1))  # Excitement/anger\n",
    "        features.append(text.count('?') / max(len(text), 1))  # Uncertainty\n",
    "        features.append(text.count('...') + text.count('‚Ä¶'))  # Hesitation\n",
    "        features.append(1 if text.isupper() and len(text) > 10 else 0)  # Shouting\n",
    "        \n",
    "        # Intensity (amplifies sentiment)\n",
    "        amplifier_count = sum(1 for w in words if w in self.intensity_amplifiers)\n",
    "        features.append(amplifier_count / max(len(words), 1))\n",
    "        \n",
    "        # Negation (reverses sentiment)\n",
    "        negation_count = sum(1 for w in words if w in self.negation_words)\n",
    "        features.append(negation_count / max(len(words), 1))\n",
    "        \n",
    "        # Emoticons (strong tone indicators)\n",
    "        pos_emoticon = sum(1 for e in self.positive_emoticons if e in text)\n",
    "        neg_emoticon = sum(1 for e in self.negative_emoticons if e in text)\n",
    "        features.append(pos_emoticon)\n",
    "        features.append(neg_emoticon)\n",
    "        \n",
    "        # Text length (detailed vs brief)\n",
    "        features.append(np.log1p(len(text)))\n",
    "        features.append(np.log1p(len(words)))\n",
    "        \n",
    "        # Capitalization emphasis\n",
    "        capital_words = sum(1 for w in words if w.isupper() and len(w) > 1)\n",
    "        features.append(capital_words / max(len(words), 1))\n",
    "        \n",
    "        # Repeated characters (emphasis: \"soooo good\")\n",
    "        repeated_chars = len(re.findall(r'(.)\\1{2,}', text_lower))\n",
    "        features.append(repeated_chars)\n",
    "        \n",
    "        # Comparative/superlative (strong opinions)\n",
    "        comparatives = {'best', 'worst', 'better', 'worse', 'great', 'terrible'}\n",
    "        comparative_count = sum(1 for w in words if w in comparatives)\n",
    "        features.append(comparative_count / max(len(words), 1))\n",
    "        \n",
    "        # Personal engagement\n",
    "        personal_pronouns = {'i', 'me', 'my', 'mine'}\n",
    "        pronoun_count = sum(1 for w in words if w in personal_pronouns)\n",
    "        features.append(pronoun_count / max(len(words), 1))\n",
    "        \n",
    "        return np.array(features, dtype=np.float32)\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Enhanced preprocessing preserving context markers\"\"\"\n",
    "        # Preserve negations and contractions\n",
    "        text = text.replace(\"n't\", \" not\")\n",
    "        text = text.replace(\"won't\", \"will not\")\n",
    "        text = text.replace(\"can't\", \"cannot\")\n",
    "        \n",
    "        # Mark intensity\n",
    "        for amplifier in self.intensity_amplifiers:\n",
    "            text = text.replace(f\" {amplifier} \", f\" INTENSE_{amplifier} \")\n",
    "        \n",
    "        # Mark negations (critical for context)\n",
    "        for negation in self.negation_words:\n",
    "            text = text.replace(f\" {negation} \", f\" NEG_{negation} \")\n",
    "        \n",
    "        # Preserve repeated punctuation\n",
    "        text = re.sub(r'!{2,}', ' MULTIEXCLAIM ', text)\n",
    "        text = re.sub(r'\\?{2,}', ' MULTIQUESTION ', text)\n",
    "        \n",
    "        return text.lower()\n",
    "    \n",
    "    def load_combined_json(self, json_path: str) -> List[Dict]:\n",
    "        \"\"\"Load reviews from JSON\"\"\"\n",
    "        print(f\"Loading {json_path}...\")\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        reviews = []\n",
    "        if 'products' in data:\n",
    "            for product in data['products']:\n",
    "                if 'reviews' in product and product['reviews']:\n",
    "                    reviews.extend(product['reviews'])\n",
    "        elif 'reviews' in data:\n",
    "            if isinstance(data['reviews'], dict):\n",
    "                for sentiment_type in ['positive', 'negative', 'neutral']:\n",
    "                    if sentiment_type in data['reviews']:\n",
    "                        reviews.extend(data['reviews'][sentiment_type])\n",
    "                if 'all' in data['reviews']:\n",
    "                    reviews.extend(data['reviews']['all'])\n",
    "            elif isinstance(data['reviews'], list):\n",
    "                reviews.extend(data['reviews'])\n",
    "        \n",
    "        print(f\"  Loaded {len(reviews)} reviews\")\n",
    "        return reviews\n",
    "    \n",
    "    def load_all_combined_files(self, directory: str = \".\") -> List[Dict]:\n",
    "        \"\"\"Load all combined JSON files\"\"\"\n",
    "        pattern = os.path.join(directory, \"*_combined_*.json\")\n",
    "        json_files = glob.glob(pattern)\n",
    "        \n",
    "        if not json_files:\n",
    "            json_files = [f for f in glob.glob(os.path.join(directory, \"*.json\")) \n",
    "                         if 'combined' in f.lower()]\n",
    "        \n",
    "        if not json_files:\n",
    "            raise FileNotFoundError(f\"No combined JSON files found in {directory}\")\n",
    "        \n",
    "        print(f\"\\nFound {len(json_files)} combined JSON file(s)\")\n",
    "        \n",
    "        all_reviews = []\n",
    "        for json_file in json_files:\n",
    "            reviews = self.load_combined_json(json_file)\n",
    "            all_reviews.extend(reviews)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen_texts = set()\n",
    "        unique_reviews = []\n",
    "        for review in all_reviews:\n",
    "            text = review.get('review_text', '')\n",
    "            if text and text not in seen_texts:\n",
    "                seen_texts.add(text)\n",
    "                unique_reviews.append(review)\n",
    "        \n",
    "        print(f\"Total unique reviews: {len(unique_reviews)}\")\n",
    "        return unique_reviews\n",
    "    \n",
    "    def prepare_data(self, reviews: List[Dict]) -> Tuple:\n",
    "        \"\"\"Prepare sequences and contextual features\"\"\"\n",
    "        texts = []\n",
    "        labels = []\n",
    "        contextual_features_list = []\n",
    "        \n",
    "        for review in reviews:\n",
    "            text = review.get('review_text', '').strip()\n",
    "            title = review.get('title', '').strip()\n",
    "            sentiment = review.get('sentiment', '').lower()\n",
    "            \n",
    "            if not text or sentiment not in self.label_map:\n",
    "                continue\n",
    "            \n",
    "            full_text = f\"{title} {text}\".strip() if title else text\n",
    "            processed_text = self.preprocess_text(full_text)\n",
    "            \n",
    "            texts.append(processed_text)\n",
    "            labels.append(self.label_map[sentiment])\n",
    "            \n",
    "            # Extract contextual features\n",
    "            context_features = self.extract_contextual_features(full_text)\n",
    "            contextual_features_list.append(context_features)\n",
    "        \n",
    "        contextual_features = np.array(contextual_features_list, dtype=np.float32)\n",
    "        \n",
    "        print(f\"\\nPrepared {len(texts)} samples\")\n",
    "        print(f\"Contextual features shape: {contextual_features.shape}\")\n",
    "        \n",
    "        # Distribution\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(\"\\nSentiment distribution:\")\n",
    "        for label_idx, count in zip(unique, counts):\n",
    "            sentiment_name = self.reverse_label_map[label_idx]\n",
    "            percentage = (count / len(labels)) * 100\n",
    "            print(f\"  {sentiment_name.capitalize()}: {count:5d} ({percentage:5.1f}%)\")\n",
    "        \n",
    "        return texts, np.array(labels), contextual_features\n",
    "    \n",
    "    def build_model(self, num_contextual_features: int):\n",
    "        \"\"\"Build BiLSTM with attention to context\"\"\"\n",
    "        print(\"\\nBuilding BiLSTM context-aware model...\")\n",
    "        \n",
    "        # Text input branch (learns sequential context)\n",
    "        text_input = Input(shape=(self.max_len,), name='text_input')\n",
    "        \n",
    "        # Embedding layer\n",
    "        embedding = Embedding(\n",
    "            input_dim=self.vocab_size,\n",
    "            output_dim=self.embedding_dim,\n",
    "            input_length=self.max_len,\n",
    "            mask_zero=True\n",
    "        )(text_input)\n",
    "        \n",
    "        # Bidirectional LSTM layers (captures forward & backward context)\n",
    "        lstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3))(embedding)\n",
    "        lstm2 = Bidirectional(LSTM(64, return_sequences=True, dropout=0.3))(lstm1)\n",
    "        \n",
    "        # Global max pooling (captures strongest signals)\n",
    "        lstm_out = GlobalMaxPooling1D()(lstm2)\n",
    "        \n",
    "        # Contextual features input (tone, negation, emphasis)\n",
    "        context_input = Input(shape=(num_contextual_features,), name='context_input')\n",
    "        \n",
    "        # Combine text understanding + contextual awareness\n",
    "        combined = Concatenate()([lstm_out, context_input])\n",
    "        \n",
    "        # Dense layers for classification\n",
    "        dense1 = Dense(128, activation='relu')(combined)\n",
    "        dropout1 = Dropout(0.4)(dense1)\n",
    "        dense2 = Dense(64, activation='relu')(dropout1)\n",
    "        dropout2 = Dropout(0.3)(dense2)\n",
    "        \n",
    "        # Output layer\n",
    "        output = Dense(3, activation='softmax', name='output')(dropout2)\n",
    "        \n",
    "        # Build model\n",
    "        model = Model(inputs=[text_input, context_input], outputs=output)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"\\nModel Architecture:\")\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, texts: List[str], labels: np.ndarray, \n",
    "              contextual_features: np.ndarray, test_size: float = 0.2):\n",
    "        \"\"\"Train BiLSTM model\"\"\"\n",
    "        \n",
    "        if not TF_AVAILABLE:\n",
    "            raise ImportError(\"TensorFlow not installed. Run: pip install tensorflow\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING BiLSTM CONTEXT-AWARE MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nüß† Why BiLSTM for context & tone:\")\n",
    "        print(\"  ‚úì Bidirectional: reads text forward AND backward\")\n",
    "        print(\"  ‚úì Understands word order and dependencies\")\n",
    "        print(\"  ‚úì Captures negation context ('not good' vs 'good')\")\n",
    "        print(\"  ‚úì Learns tone patterns from sequence\")\n",
    "        print(\"  ‚úì Combines with explicit tone features\")\n",
    "        print()\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test, ctx_train, ctx_test = train_test_split(\n",
    "            texts, labels, contextual_features,\n",
    "            test_size=test_size, random_state=42, stratify=labels\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set: {len(X_train)} samples\")\n",
    "        print(f\"Test set: {len(X_test)} samples\")\n",
    "        \n",
    "        # Tokenize text\n",
    "        print(\"\\nTokenizing text...\")\n",
    "        self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token='<OOV>')\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        \n",
    "        X_train_seq = self.tokenizer.texts_to_sequences(X_train)\n",
    "        X_test_seq = self.tokenizer.texts_to_sequences(X_test)\n",
    "        \n",
    "        # Pad sequences\n",
    "        X_train_pad = pad_sequences(X_train_seq, maxlen=self.max_len, padding='post')\n",
    "        X_test_pad = pad_sequences(X_test_seq, maxlen=self.max_len, padding='post')\n",
    "        \n",
    "        print(f\"  Vocabulary size: {len(self.tokenizer.word_index)}\")\n",
    "        print(f\"  Sequence shape: {X_train_pad.shape}\")\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model(contextual_features.shape[1])\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=0.00001,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        print(\"\\nTraining model (this may take 5-15 minutes)...\")\n",
    "        print(\"Progress:\")\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            {'text_input': X_train_pad, 'context_input': ctx_train},\n",
    "            y_train,\n",
    "            validation_split=0.15,\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úì Training completed!\")\n",
    "        \n",
    "        # Evaluate\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATION ON TEST SET\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        y_pred_probs = self.model.predict(\n",
    "            {'text_input': X_test_pad, 'context_input': ctx_test},\n",
    "            verbose=0\n",
    "        )\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"\\nüéØ Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(\n",
    "            y_test, y_pred,\n",
    "            target_names=['Negative', 'Neutral', 'Positive'],\n",
    "            digits=4\n",
    "        ))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(\"                Predicted\")\n",
    "        print(\"              Neg   Neu   Pos\")\n",
    "        for i, (label, row) in enumerate(zip(['Negative', 'Neutral', 'Positive'], cm)):\n",
    "            print(f\"Actual {label:8s}  {row[0]:4d}  {row[1]:4d}  {row[2]:4d}\")\n",
    "        \n",
    "        self.plot_confusion_matrix(cm, ['Negative', 'Neutral', 'Positive'])\n",
    "        self.plot_training_history(history)\n",
    "        \n",
    "        return accuracy, history\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, labels):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels, yticklabels=labels)\n",
    "        plt.title('Confusion Matrix - BiLSTM')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f'confusion_matrix_bilstm_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nConfusion matrix saved to {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Accuracy\n",
    "        ax1.plot(history.history['accuracy'], label='Train')\n",
    "        ax1.plot(history.history['val_accuracy'], label='Validation')\n",
    "        ax1.set_title('Model Accuracy')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss\n",
    "        ax2.plot(history.history['loss'], label='Train')\n",
    "        ax2.plot(history.history['val_loss'], label='Validation')\n",
    "        ax2.set_title('Model Loss')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f'training_history_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Training history saved to {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def save_model(self, model_path: str = None):\n",
    "        \"\"\"Save model and tokenizer\"\"\"\n",
    "        if model_path is None:\n",
    "            model_path = f\"walmart_sentiment_bilstm_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Save Keras model\n",
    "        self.model.save(f\"{model_path}.keras\")\n",
    "        print(f\"\\nKeras model saved to {model_path}.keras\")\n",
    "        \n",
    "        # Save tokenizer and metadata\n",
    "        metadata = {\n",
    "            'tokenizer': self.tokenizer,\n",
    "            'label_map': self.label_map,\n",
    "            'reverse_label_map': self.reverse_label_map,\n",
    "            'max_len': self.max_len,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'intensity_amplifiers': self.intensity_amplifiers,\n",
    "            'negation_words': self.negation_words,\n",
    "            'positive_emoticons': self.positive_emoticons,\n",
    "            'negative_emoticons': self.negative_emoticons,\n",
    "            'model_type': 'bilstm',\n",
    "            'trained_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(f\"{model_path}_metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        print(f\"Metadata saved to {model_path}_metadata.pkl\")\n",
    "        \n",
    "        with open(\"latest_model.txt\", 'w') as f:\n",
    "            f.write(model_path)\n",
    "        print(\"Latest model path saved to latest_model.txt\")\n",
    "        \n",
    "        return model_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BiLSTM CONTEXT-AWARE SENTIMENT TRAINER\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüß† Best for understanding context and tone:\")\n",
    "    print(\"  ‚Ä¢ Negation handling: 'not good' vs 'good'\")\n",
    "    print(\"  ‚Ä¢ Word order matters: 'good but expensive'\")\n",
    "    print(\"  ‚Ä¢ Tone detection: '!!!' vs '...'\")\n",
    "    print(\"  ‚Ä¢ Sequential dependencies\")\n",
    "    print()\n",
    "    \n",
    "    if not TF_AVAILABLE:\n",
    "        print(\"‚ùå TensorFlow not installed!\")\n",
    "        print(\"\\nInstall with: pip install tensorflow\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        trainer = BiLSTMContextTrainer()\n",
    "        \n",
    "        directory = input(\"Enter directory with JSON files (or Enter for current): \").strip()\n",
    "        if not directory:\n",
    "            directory = \".\"\n",
    "        \n",
    "        reviews = trainer.load_all_combined_files(directory)\n",
    "        \n",
    "        if len(reviews) < 500:\n",
    "            print(f\"\\n‚ö†Ô∏è  Warning: Only {len(reviews)} reviews. BiLSTM works best with 5000+\")\n",
    "            confirm = input(\"Continue anyway? (y/n): \").strip().lower()\n",
    "            if confirm != 'y':\n",
    "                return\n",
    "        \n",
    "        texts, labels, contextual_features = trainer.prepare_data(reviews)\n",
    "        \n",
    "        if len(texts) == 0:\n",
    "            print(\"\\n‚ùå No valid reviews found!\")\n",
    "            return\n",
    "        \n",
    "        test_size = 0.2\n",
    "        test_input = input(f\"\\nTest set size (default 0.2): \").strip()\n",
    "        if test_input:\n",
    "            try:\n",
    "                test_size = float(test_input)\n",
    "                if not 0 < test_size < 1:\n",
    "                    test_size = 0.2\n",
    "            except:\n",
    "                test_size = 0.2\n",
    "        \n",
    "        accuracy, history = trainer.train(texts, labels, contextual_features, test_size)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SAVING MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        model_path = trainer.save_model()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nüéØ Final Accuracy: {accuracy*100:.2f}%\")\n",
    "        print(f\"üì¶ Model: {model_path}.keras\")\n",
    "        print(f\"\\nBiLSTM advantages:\")\n",
    "        print(f\"  ‚úì Understands 'not good' ‚â† 'good'\")\n",
    "        print(f\"  ‚úì Detects tone from punctuation\")\n",
    "        print(f\"  ‚úì Captures word order context\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nTraining interrupted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
