{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6f06b-2daf-46e0-a396-b8e616478939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BiLSTM CONTEXT-AWARE SENTIMENT TRAINER\n",
      "============================================================\n",
      "\n",
      "🧠 Best for understanding context and tone:\n",
      "  • Negation handling: 'not good' vs 'good'\n",
      "  • Word order matters: 'good but expensive'\n",
      "  • Tone detection: '!!!' vs '...'\n",
      "  • Sequential dependencies\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter directory with JSON files (or Enter for current):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1 combined JSON file(s)\n",
      "Loading .\\_combined_.json...\n",
      "  Loaded 9222 reviews\n",
      "Total unique reviews: 9222\n",
      "\n",
      "Prepared 9222 samples\n",
      "Contextual features shape: (9222, 14)\n",
      "\n",
      "Sentiment distribution:\n",
      "  Negative:  3074 ( 33.3%)\n",
      "  Neutral:  3074 ( 33.3%)\n",
      "  Positive:  3074 ( 33.3%)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set size (default 0.2):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING BiLSTM CONTEXT-AWARE MODEL\n",
      "============================================================\n",
      "\n",
      "🧠 Why BiLSTM for context & tone:\n",
      "  ✓ Bidirectional: reads text forward AND backward\n",
      "  ✓ Understands word order and dependencies\n",
      "  ✓ Captures negation context ('not good' vs 'good')\n",
      "  ✓ Learns tone patterns from sequence\n",
      "  ✓ Combines with explicit tone features\n",
      "\n",
      "Training set: 7377 samples\n",
      "Test set: 1845 samples\n",
      "\n",
      "Tokenizing text...\n",
      "  Vocabulary size: 12634\n",
      "  Sequence shape: (7377, 150)\n",
      "\n",
      "Building BiLSTM context-aware model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:970: UserWarning: Layer 'global_max_pooling1d' (of type GlobalMaxPooling1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                               │                           │                 │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)               │                           │                 │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ context_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">142</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                               │                           │                 │ context_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,304</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m100\u001b[0m)          │       \u001b[38;5;34m1,000,000\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m256\u001b[0m)          │         \u001b[38;5;34m234,496\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                               │                           │                 │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │         \u001b[38;5;34m164,352\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)               │                           │                 │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ context_input (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m142\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ global_max_pooling1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                               │                           │                 │ context_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m18,304\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │           \u001b[38;5;34m8,256\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 │             \u001b[38;5;34m195\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,425,603</span> (5.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,425,603\u001b[0m (5.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,425,603</span> (5.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,425,603\u001b[0m (5.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model (this may take 5-15 minutes)...\n",
      "Progress:\n",
      "Epoch 1/30\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 248ms/step - accuracy: 0.5027 - loss: 0.9533 - val_accuracy: 0.6251 - val_loss: 0.7990 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m179/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m3s\u001b[0m 232ms/step - accuracy: 0.6725 - loss: 0.7252"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import (Embedding, LSTM, Bidirectional, Dense, \n",
    "                                          Dropout, Input, Concatenate, GlobalMaxPooling1D)\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"Warning: TensorFlow not installed. Install with: pip install tensorflow\")\n",
    "\n",
    "\n",
    "class BiLSTMContextTrainer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize BiLSTM with context and tone awareness\"\"\"\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.max_len = 150  # Optimal for reviews\n",
    "        self.vocab_size = 10000\n",
    "        self.embedding_dim = 100\n",
    "        \n",
    "        self.label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "        self.reverse_label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "        \n",
    "        # Tone and context markers\n",
    "        self.intensity_amplifiers = {\n",
    "            'very', 'extremely', 'incredibly', 'absolutely', 'totally',\n",
    "            'completely', 'utterly', 'highly', 'really', 'so', 'super'\n",
    "        }\n",
    "        \n",
    "        self.negation_words = {\n",
    "            'not', 'no', 'never', 'nothing', 'nowhere', 'neither', 'nobody',\n",
    "            'none', 'hardly', 'scarcely', 'barely', \"n't\", 'cannot', 'cant', 'won\\'t'\n",
    "        }\n",
    "        \n",
    "        self.positive_emoticons = [':)', ':-)', ':D', ':-D', ':P', '^_^', '😊', '😃', '👍', '❤️']\n",
    "        self.negative_emoticons = [':(', ':-(', ':[', ':-[', ':/',':-/', '😢', '😞', '👎', '💔']\n",
    "    \n",
    "    def extract_contextual_features(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Extract tone and context features for BiLSTM\"\"\"\n",
    "        features = []\n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        # Tone indicators\n",
    "        features.append(text.count('!') / max(len(text), 1))  # Excitement/anger\n",
    "        features.append(text.count('?') / max(len(text), 1))  # Uncertainty\n",
    "        features.append(text.count('...') + text.count('…'))  # Hesitation\n",
    "        features.append(1 if text.isupper() and len(text) > 10 else 0)  # Shouting\n",
    "        \n",
    "        # Intensity (amplifies sentiment)\n",
    "        amplifier_count = sum(1 for w in words if w in self.intensity_amplifiers)\n",
    "        features.append(amplifier_count / max(len(words), 1))\n",
    "        \n",
    "        # Negation (reverses sentiment)\n",
    "        negation_count = sum(1 for w in words if w in self.negation_words)\n",
    "        features.append(negation_count / max(len(words), 1))\n",
    "        \n",
    "        # Emoticons (strong tone indicators)\n",
    "        pos_emoticon = sum(1 for e in self.positive_emoticons if e in text)\n",
    "        neg_emoticon = sum(1 for e in self.negative_emoticons if e in text)\n",
    "        features.append(pos_emoticon)\n",
    "        features.append(neg_emoticon)\n",
    "        \n",
    "        # Text length (detailed vs brief)\n",
    "        features.append(np.log1p(len(text)))\n",
    "        features.append(np.log1p(len(words)))\n",
    "        \n",
    "        # Capitalization emphasis\n",
    "        capital_words = sum(1 for w in words if w.isupper() and len(w) > 1)\n",
    "        features.append(capital_words / max(len(words), 1))\n",
    "        \n",
    "        # Repeated characters (emphasis: \"soooo good\")\n",
    "        repeated_chars = len(re.findall(r'(.)\\1{2,}', text_lower))\n",
    "        features.append(repeated_chars)\n",
    "        \n",
    "        # Comparative/superlative (strong opinions)\n",
    "        comparatives = {'best', 'worst', 'better', 'worse', 'great', 'terrible'}\n",
    "        comparative_count = sum(1 for w in words if w in comparatives)\n",
    "        features.append(comparative_count / max(len(words), 1))\n",
    "        \n",
    "        # Personal engagement\n",
    "        personal_pronouns = {'i', 'me', 'my', 'mine'}\n",
    "        pronoun_count = sum(1 for w in words if w in personal_pronouns)\n",
    "        features.append(pronoun_count / max(len(words), 1))\n",
    "        \n",
    "        return np.array(features, dtype=np.float32)\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Enhanced preprocessing preserving context markers\"\"\"\n",
    "        # Preserve negations and contractions\n",
    "        text = text.replace(\"n't\", \" not\")\n",
    "        text = text.replace(\"won't\", \"will not\")\n",
    "        text = text.replace(\"can't\", \"cannot\")\n",
    "        \n",
    "        # Mark intensity\n",
    "        for amplifier in self.intensity_amplifiers:\n",
    "            text = text.replace(f\" {amplifier} \", f\" INTENSE_{amplifier} \")\n",
    "        \n",
    "        # Mark negations (critical for context)\n",
    "        for negation in self.negation_words:\n",
    "            text = text.replace(f\" {negation} \", f\" NEG_{negation} \")\n",
    "        \n",
    "        # Preserve repeated punctuation\n",
    "        text = re.sub(r'!{2,}', ' MULTIEXCLAIM ', text)\n",
    "        text = re.sub(r'\\?{2,}', ' MULTIQUESTION ', text)\n",
    "        \n",
    "        return text.lower()\n",
    "    \n",
    "    def load_combined_json(self, json_path: str) -> List[Dict]:\n",
    "        \"\"\"Load reviews from JSON\"\"\"\n",
    "        print(f\"Loading {json_path}...\")\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        reviews = []\n",
    "        if 'products' in data:\n",
    "            for product in data['products']:\n",
    "                if 'reviews' in product and product['reviews']:\n",
    "                    reviews.extend(product['reviews'])\n",
    "        elif 'reviews' in data:\n",
    "            if isinstance(data['reviews'], dict):\n",
    "                for sentiment_type in ['positive', 'negative', 'neutral']:\n",
    "                    if sentiment_type in data['reviews']:\n",
    "                        reviews.extend(data['reviews'][sentiment_type])\n",
    "                if 'all' in data['reviews']:\n",
    "                    reviews.extend(data['reviews']['all'])\n",
    "            elif isinstance(data['reviews'], list):\n",
    "                reviews.extend(data['reviews'])\n",
    "        \n",
    "        print(f\"  Loaded {len(reviews)} reviews\")\n",
    "        return reviews\n",
    "    \n",
    "    def load_all_combined_files(self, directory: str = \".\") -> List[Dict]:\n",
    "        \"\"\"Load all combined JSON files\"\"\"\n",
    "        pattern = os.path.join(directory, \"*_combined_*.json\")\n",
    "        json_files = glob.glob(pattern)\n",
    "        \n",
    "        if not json_files:\n",
    "            json_files = [f for f in glob.glob(os.path.join(directory, \"*.json\")) \n",
    "                         if 'combined' in f.lower()]\n",
    "        \n",
    "        if not json_files:\n",
    "            raise FileNotFoundError(f\"No combined JSON files found in {directory}\")\n",
    "        \n",
    "        print(f\"\\nFound {len(json_files)} combined JSON file(s)\")\n",
    "        \n",
    "        all_reviews = []\n",
    "        for json_file in json_files:\n",
    "            reviews = self.load_combined_json(json_file)\n",
    "            all_reviews.extend(reviews)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen_texts = set()\n",
    "        unique_reviews = []\n",
    "        for review in all_reviews:\n",
    "            text = review.get('review_text', '')\n",
    "            if text and text not in seen_texts:\n",
    "                seen_texts.add(text)\n",
    "                unique_reviews.append(review)\n",
    "        \n",
    "        print(f\"Total unique reviews: {len(unique_reviews)}\")\n",
    "        return unique_reviews\n",
    "    \n",
    "    def prepare_data(self, reviews: List[Dict]) -> Tuple:\n",
    "        \"\"\"Prepare sequences and contextual features\"\"\"\n",
    "        texts = []\n",
    "        labels = []\n",
    "        contextual_features_list = []\n",
    "        \n",
    "        for review in reviews:\n",
    "            text = review.get('review_text', '').strip()\n",
    "            title = review.get('title', '').strip()\n",
    "            sentiment = review.get('sentiment', '').lower()\n",
    "            \n",
    "            if not text or sentiment not in self.label_map:\n",
    "                continue\n",
    "            \n",
    "            full_text = f\"{title} {text}\".strip() if title else text\n",
    "            processed_text = self.preprocess_text(full_text)\n",
    "            \n",
    "            texts.append(processed_text)\n",
    "            labels.append(self.label_map[sentiment])\n",
    "            \n",
    "            # Extract contextual features\n",
    "            context_features = self.extract_contextual_features(full_text)\n",
    "            contextual_features_list.append(context_features)\n",
    "        \n",
    "        contextual_features = np.array(contextual_features_list, dtype=np.float32)\n",
    "        \n",
    "        print(f\"\\nPrepared {len(texts)} samples\")\n",
    "        print(f\"Contextual features shape: {contextual_features.shape}\")\n",
    "        \n",
    "        # Distribution\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(\"\\nSentiment distribution:\")\n",
    "        for label_idx, count in zip(unique, counts):\n",
    "            sentiment_name = self.reverse_label_map[label_idx]\n",
    "            percentage = (count / len(labels)) * 100\n",
    "            print(f\"  {sentiment_name.capitalize()}: {count:5d} ({percentage:5.1f}%)\")\n",
    "        \n",
    "        return texts, np.array(labels), contextual_features\n",
    "    \n",
    "    def build_model(self, num_contextual_features: int):\n",
    "        \"\"\"Build BiLSTM with attention to context\"\"\"\n",
    "        print(\"\\nBuilding BiLSTM context-aware model...\")\n",
    "        \n",
    "        # Text input branch (learns sequential context)\n",
    "        text_input = Input(shape=(self.max_len,), name='text_input')\n",
    "        \n",
    "        # Embedding layer\n",
    "        embedding = Embedding(\n",
    "            input_dim=self.vocab_size,\n",
    "            output_dim=self.embedding_dim,\n",
    "            input_length=self.max_len,\n",
    "            mask_zero=True\n",
    "        )(text_input)\n",
    "        \n",
    "        # Bidirectional LSTM layers (captures forward & backward context)\n",
    "        lstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3))(embedding)\n",
    "        lstm2 = Bidirectional(LSTM(64, return_sequences=True, dropout=0.3))(lstm1)\n",
    "        \n",
    "        # Global max pooling (captures strongest signals)\n",
    "        lstm_out = GlobalMaxPooling1D()(lstm2)\n",
    "        \n",
    "        # Contextual features input (tone, negation, emphasis)\n",
    "        context_input = Input(shape=(num_contextual_features,), name='context_input')\n",
    "        \n",
    "        # Combine text understanding + contextual awareness\n",
    "        combined = Concatenate()([lstm_out, context_input])\n",
    "        \n",
    "        # Dense layers for classification\n",
    "        dense1 = Dense(128, activation='relu')(combined)\n",
    "        dropout1 = Dropout(0.4)(dense1)\n",
    "        dense2 = Dense(64, activation='relu')(dropout1)\n",
    "        dropout2 = Dropout(0.3)(dense2)\n",
    "        \n",
    "        # Output layer\n",
    "        output = Dense(3, activation='softmax', name='output')(dropout2)\n",
    "        \n",
    "        # Build model\n",
    "        model = Model(inputs=[text_input, context_input], outputs=output)\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"\\nModel Architecture:\")\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, texts: List[str], labels: np.ndarray, \n",
    "              contextual_features: np.ndarray, test_size: float = 0.2):\n",
    "        \"\"\"Train BiLSTM model\"\"\"\n",
    "        \n",
    "        if not TF_AVAILABLE:\n",
    "            raise ImportError(\"TensorFlow not installed. Run: pip install tensorflow\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING BiLSTM CONTEXT-AWARE MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\n🧠 Why BiLSTM for context & tone:\")\n",
    "        print(\"  ✓ Bidirectional: reads text forward AND backward\")\n",
    "        print(\"  ✓ Understands word order and dependencies\")\n",
    "        print(\"  ✓ Captures negation context ('not good' vs 'good')\")\n",
    "        print(\"  ✓ Learns tone patterns from sequence\")\n",
    "        print(\"  ✓ Combines with explicit tone features\")\n",
    "        print()\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test, ctx_train, ctx_test = train_test_split(\n",
    "            texts, labels, contextual_features,\n",
    "            test_size=test_size, random_state=42, stratify=labels\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set: {len(X_train)} samples\")\n",
    "        print(f\"Test set: {len(X_test)} samples\")\n",
    "        \n",
    "        # Tokenize text\n",
    "        print(\"\\nTokenizing text...\")\n",
    "        self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token='<OOV>')\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        \n",
    "        X_train_seq = self.tokenizer.texts_to_sequences(X_train)\n",
    "        X_test_seq = self.tokenizer.texts_to_sequences(X_test)\n",
    "        \n",
    "        # Pad sequences\n",
    "        X_train_pad = pad_sequences(X_train_seq, maxlen=self.max_len, padding='post')\n",
    "        X_test_pad = pad_sequences(X_test_seq, maxlen=self.max_len, padding='post')\n",
    "        \n",
    "        print(f\"  Vocabulary size: {len(self.tokenizer.word_index)}\")\n",
    "        print(f\"  Sequence shape: {X_train_pad.shape}\")\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model(contextual_features.shape[1])\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=0.00001,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        print(\"\\nTraining model (this may take 5-15 minutes)...\")\n",
    "        print(\"Progress:\")\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            {'text_input': X_train_pad, 'context_input': ctx_train},\n",
    "            y_train,\n",
    "            validation_split=0.15,\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\n✓ Training completed!\")\n",
    "        \n",
    "        # Evaluate\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATION ON TEST SET\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        y_pred_probs = self.model.predict(\n",
    "            {'text_input': X_test_pad, 'context_input': ctx_test},\n",
    "            verbose=0\n",
    "        )\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"\\n🎯 Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(\n",
    "            y_test, y_pred,\n",
    "            target_names=['Negative', 'Neutral', 'Positive'],\n",
    "            digits=4\n",
    "        ))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(\"                Predicted\")\n",
    "        print(\"              Neg   Neu   Pos\")\n",
    "        for i, (label, row) in enumerate(zip(['Negative', 'Neutral', 'Positive'], cm)):\n",
    "            print(f\"Actual {label:8s}  {row[0]:4d}  {row[1]:4d}  {row[2]:4d}\")\n",
    "        \n",
    "        self.plot_confusion_matrix(cm, ['Negative', 'Neutral', 'Positive'])\n",
    "        self.plot_training_history(history)\n",
    "        \n",
    "        return accuracy, history\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, labels):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels, yticklabels=labels)\n",
    "        plt.title('Confusion Matrix - BiLSTM')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f'confusion_matrix_bilstm_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nConfusion matrix saved to {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Accuracy\n",
    "        ax1.plot(history.history['accuracy'], label='Train')\n",
    "        ax1.plot(history.history['val_accuracy'], label='Validation')\n",
    "        ax1.set_title('Model Accuracy')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss\n",
    "        ax2.plot(history.history['loss'], label='Train')\n",
    "        ax2.plot(history.history['val_loss'], label='Validation')\n",
    "        ax2.set_title('Model Loss')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f'training_history_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Training history saved to {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def save_model(self, model_path: str = None):\n",
    "        \"\"\"Save model and tokenizer\"\"\"\n",
    "        if model_path is None:\n",
    "            model_path = f\"walmart_sentiment_bilstm_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Save Keras model\n",
    "        self.model.save(f\"{model_path}.keras\")\n",
    "        print(f\"\\nKeras model saved to {model_path}.keras\")\n",
    "        \n",
    "        # Save tokenizer and metadata\n",
    "        metadata = {\n",
    "            'tokenizer': self.tokenizer,\n",
    "            'label_map': self.label_map,\n",
    "            'reverse_label_map': self.reverse_label_map,\n",
    "            'max_len': self.max_len,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'intensity_amplifiers': self.intensity_amplifiers,\n",
    "            'negation_words': self.negation_words,\n",
    "            'positive_emoticons': self.positive_emoticons,\n",
    "            'negative_emoticons': self.negative_emoticons,\n",
    "            'model_type': 'bilstm',\n",
    "            'trained_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(f\"{model_path}_metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        print(f\"Metadata saved to {model_path}_metadata.pkl\")\n",
    "        \n",
    "        with open(\"latest_model.txt\", 'w') as f:\n",
    "            f.write(model_path)\n",
    "        print(\"Latest model path saved to latest_model.txt\")\n",
    "        \n",
    "        return model_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BiLSTM CONTEXT-AWARE SENTIMENT TRAINER\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n🧠 Best for understanding context and tone:\")\n",
    "    print(\"  • Negation handling: 'not good' vs 'good'\")\n",
    "    print(\"  • Word order matters: 'good but expensive'\")\n",
    "    print(\"  • Tone detection: '!!!' vs '...'\")\n",
    "    print(\"  • Sequential dependencies\")\n",
    "    print()\n",
    "    \n",
    "    if not TF_AVAILABLE:\n",
    "        print(\"❌ TensorFlow not installed!\")\n",
    "        print(\"\\nInstall with: pip install tensorflow\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        trainer = BiLSTMContextTrainer()\n",
    "        \n",
    "        directory = input(\"Enter directory with JSON files (or Enter for current): \").strip()\n",
    "        if not directory:\n",
    "            directory = \".\"\n",
    "        \n",
    "        reviews = trainer.load_all_combined_files(directory)\n",
    "        \n",
    "        if len(reviews) < 500:\n",
    "            print(f\"\\n⚠️  Warning: Only {len(reviews)} reviews. BiLSTM works best with 5000+\")\n",
    "            confirm = input(\"Continue anyway? (y/n): \").strip().lower()\n",
    "            if confirm != 'y':\n",
    "                return\n",
    "        \n",
    "        texts, labels, contextual_features = trainer.prepare_data(reviews)\n",
    "        \n",
    "        if len(texts) == 0:\n",
    "            print(\"\\n❌ No valid reviews found!\")\n",
    "            return\n",
    "        \n",
    "        test_size = 0.2\n",
    "        test_input = input(f\"\\nTest set size (default 0.2): \").strip()\n",
    "        if test_input:\n",
    "            try:\n",
    "                test_size = float(test_input)\n",
    "                if not 0 < test_size < 1:\n",
    "                    test_size = 0.2\n",
    "            except:\n",
    "                test_size = 0.2\n",
    "        \n",
    "        accuracy, history = trainer.train(texts, labels, contextual_features, test_size)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SAVING MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        model_path = trainer.save_model()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✅ TRAINING COMPLETED!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\n🎯 Final Accuracy: {accuracy*100:.2f}%\")\n",
    "        print(f\"📦 Model: {model_path}.keras\")\n",
    "        print(f\"\\nBiLSTM advantages:\")\n",
    "        print(f\"  ✓ Understands 'not good' ≠ 'good'\")\n",
    "        print(f\"  ✓ Detects tone from punctuation\")\n",
    "        print(f\"  ✓ Captures word order context\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nTraining interrupted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f9cc6-3961-414f-ae0c-810bab925b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
